{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'src'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deque\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m        \n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfiltering\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m filtering_data\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfourier\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m fourier_transform\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msignal\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m find_peaks, convolve, welch\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'src'"
     ]
    }
   ],
   "source": [
    "from obspy import Trace, UTCDateTime, Stream\n",
    "import matplotlib.pyplot as plt\n",
    "from multiprocessing import Queue\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import logging\n",
    "import logging.handlers\n",
    "import time\n",
    "from collections import deque\n",
    "import pandas as pd        \n",
    "from src.filtering import filtering_data\n",
    "from src.fourier import fourier_transform\n",
    "from scipy.signal import find_peaks, convolve, welch\n",
    "from obspy.signal.trigger import classic_sta_lta, recursive_sta_lta, z_detect\n",
    "from scipy.fft import fft, fftfreq\n",
    "from scipy.integrate import simpson\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging (if not already configured in your main module)\n",
    "logging.basicConfig(\n",
    "        level=logging.DEBUG,\n",
    "        format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "def gaussian_window(size, std):\n",
    "    \"\"\"Create a Gaussian window.\"\"\"\n",
    "    return np.exp(-0.5 * (np.linspace(-1, 1, size) / std) ** 2)\n",
    "\n",
    "def get_peakpeak_time_psd(cft, start_index, end_index, sampling_rate):\n",
    "    data = cft[start_index:end_index]\n",
    "    f,psd = welch(data,nperseg=64,fs=sampling_rate,nfft=64)\n",
    "    f = f[2:]; psd=psd[2:] #avoid 0\n",
    "    maxfreq = f[np.argmax(psd)]\n",
    "    timespan = 1/(2*maxfreq)\n",
    "    #plt.plot(f, psd)\n",
    "    #plt.show()\n",
    "    return timespan\n",
    "\n",
    "def main_processing(trace, gap, dominant_frequency):\n",
    "                    \n",
    "    # Get the sampling rate\n",
    "    sampling_rate = trace.stats.sampling_rate\n",
    "    logging.error(f\"sampling_rate {sampling_rate}.\")\n",
    "    # Method for CFT\n",
    "    method = \"recursive_sta_lta\"\n",
    "\n",
    "    if method == \"z_detect\":\n",
    "        cft = z_detect(trace.data, int(10 * sampling_rate))\n",
    "    elif method == \"recursive_sta_lta\":\n",
    "        cft = recursive_sta_lta(trace.data, int(5 * sampling_rate), int(10 * sampling_rate))\n",
    "    else:\n",
    "        cft = classic_sta_lta(trace.data, int(5 * sampling_rate), int(10 * sampling_rate))\n",
    "\n",
    "    # CFT setup: ignore start and end portions\n",
    "    end_index = len(cft)\n",
    "    start_index=0\n",
    "\n",
    "\n",
    "    # Slice the CFT and raw data\n",
    "    cft_sliced = cft[start_index:end_index]\n",
    "    time_vector = np.arange(len(cft_sliced)) / sampling_rate\n",
    "    raw_data_sliced = trace.data[start_index:end_index]\n",
    "    time_vector_raw = time_vector[:len(raw_data_sliced)]\n",
    "\n",
    "    # Smooth the CFT using a Gaussian filter\n",
    "    window_size = int(0.5 * sampling_rate)\n",
    "    std_dev = window_size / 10\n",
    "    gauss_window = gaussian_window(window_size, std_dev)\n",
    "    smoothed_cft = convolve(cft_sliced, gauss_window / np.sum(gauss_window), mode='same')\n",
    "    logging.error(\"Ran Gauss nd smoothed.\")\n",
    "    # Adaptive thresholding based on percentiles\n",
    "    low_percentile = 90\n",
    "    high_percentile = 99\n",
    "    low_threshold = np.percentile(smoothed_cft, low_percentile)\n",
    "    high_threshold = np.percentile(smoothed_cft, high_percentile)\n",
    "\n",
    "    # Peak detection (dual-pass)\n",
    "    # First pass: Detect high-prominence peaks\n",
    "    high_prominence_peaks, _ = find_peaks(\n",
    "        smoothed_cft,\n",
    "        height=high_threshold,\n",
    "        distance=int(1 * sampling_rate),\n",
    "        prominence=0.5\n",
    "    )\n",
    "\n",
    "    # Second pass: Detect lower prominence peaks near the high-prominence peaks\n",
    "    additional_peaks, _ = find_peaks(\n",
    "        smoothed_cft,\n",
    "        height=low_threshold,\n",
    "        distance=int(0.5 * sampling_rate),\n",
    "        prominence=0.2\n",
    "    )\n",
    "\n",
    "    # Combine the peaks without duplicates\n",
    "    combined_peaks = np.unique(np.concatenate((high_prominence_peaks, additional_peaks)))\n",
    "\n",
    "    # Identify valleys between the peaks\n",
    "    valleys = []\n",
    "    valley_positions = []\n",
    "    for i in range(len(combined_peaks) - 1):\n",
    "        peak_start = combined_peaks[i]\n",
    "        peak_end = combined_peaks[i + 1]\n",
    "        region = smoothed_cft[peak_start:peak_end]\n",
    "        valley = np.min(region)\n",
    "        valley_index = peak_start + np.argmin(region)\n",
    "        valleys.append(valley)\n",
    "        valley_positions.append(valley_index)\n",
    "\n",
    "    # Calculate amplitudes with adaptive window sizing\n",
    "    max_amplitudes = []\n",
    "    max_positive_peaks = []\n",
    "    max_negative_peaks = []\n",
    "    windows = []\n",
    "    window_duration = 10\n",
    "    window_samples = int(window_duration * sampling_rate)\n",
    "    std_threshold = np.std(smoothed_cft[:int(5 * sampling_rate)]) * 1e-2  # Noise-adjusted flatness threshold\n",
    "    details = {}\n",
    "    for i, peak in enumerate(combined_peaks):\n",
    "        # Determine valley bounds around each peak\n",
    "        if i == 0:\n",
    "            valley_coord_before = 0\n",
    "        else:\n",
    "            valley_coord_before = valley_positions[i - 1]\n",
    "\n",
    "        if i == len(combined_peaks) - 1:\n",
    "            valley_coord_next = len(time_vector)\n",
    "        else:\n",
    "            valley_coord_next = valley_positions[i]\n",
    "\n",
    "        start_window_index = max(valley_coord_before, peak - window_samples // 2)\n",
    "        end_window_index = min(valley_coord_next, peak + window_samples // 2)\n",
    "        start_window_index = max(0, min(len(raw_data_sliced) - 1, start_window_index))\n",
    "        end_window_index = max(0, min(len(raw_data_sliced), end_window_index))-1\n",
    "        windowed_raw_data = raw_data_sliced[start_window_index:end_window_index]\n",
    "\n",
    "        if windowed_raw_data.size > 0:\n",
    "            max_amplitude = np.max(windowed_raw_data)\n",
    "            min_amplitude = np.min(windowed_raw_data)\n",
    "        else:\n",
    "            max_amplitude = 0\n",
    "            min_amplitude = 0\n",
    "\n",
    "        max_amplitudes.append(max_amplitude - min_amplitude)\n",
    "        max_positive_peaks.append(max_amplitude if max_amplitude > 0 else 0)\n",
    "        max_negative_peaks.append(abs(min_amplitude) if min_amplitude < 0 else 0)\n",
    "        \n",
    "        if start_window_index < len(time_vector_raw) and end_window_index < len(time_vector_raw):\n",
    "            windows.append((time_vector_raw[start_window_index], time_vector_raw[end_window_index]))\n",
    "        else:\n",
    "            # Handle the error case (maybe skip this window or log a warning)\n",
    "            logging.error(f\"Indices {start_window_index}, {end_window_index} are out of bounds.\")\n",
    "    \n",
    "        peak2peak = get_peakpeak_time_psd(raw_data_sliced, start_window_index, end_window_index, sampling_rate)\n",
    "\n",
    "        #print(f\"Peak2Peak for peak {i+1}: {peak2peak}\")\n",
    "        details[i+1]= {\"peak2peak\": peak2peak}\n",
    "    # Plotting\n",
    "    #fig, axs = plt.subplots(2, 1, figsize=(30, 10), sharex=True)\n",
    "\n",
    "    # Plot smoothed CFT and peaks/valleys\n",
    "    #axs[0].plot(time_vector, smoothed_cft, label='Smoothed CFT', color='green')\n",
    "    #axs[0].scatter(time_vector[combined_peaks], smoothed_cft[combined_peaks], color='red', label='Detected Peaks', zorder=5)\n",
    "    #axs[0].scatter(time_vector[valley_positions], smoothed_cft[valley_positions], color='pink', label='Detected Valleys', zorder=5)\n",
    "\n",
    "    # Plot raw data with amplitude markers\n",
    "    #axs[1].plot(time_vector_raw, raw_data_sliced, label='Raw Data', color='blue')\n",
    "    \n",
    "    for i, peak in enumerate(combined_peaks):\n",
    "        peak_time = time_vector_raw[peak]\n",
    "        max_pos = max_positive_peaks[i]\n",
    "        max_neg = -max_negative_peaks[i]\n",
    "    \n",
    "        # Print the absolute amplitude for each peak-window\n",
    "        absolute_amplitude = max(max_pos, abs(max_neg))\n",
    "        \n",
    "        # Assuming raw_data_sliced is your velocity data\n",
    "        max_velocity_amplitude = absolute_amplitude  # Get maximum absolute velocity amplitude\n",
    "\n",
    "        # Calculate displacement amplitude\n",
    "        displacement_amplitude = max_velocity_amplitude / (2 * np.pi * dominant_frequency)\n",
    "        \n",
    "        details[i+1][\"Vel Amp (m/s)\"] = max_velocity_amplitude\n",
    "        details[i+1][\"Disp Amp (m)\"] = displacement_amplitude\n",
    "\n",
    "    # Find the maximum amplitude\n",
    "    max_amplitude = np.max(np.abs(trace.data))\n",
    "\n",
    "    # Perform FFT to find frequencies\n",
    "    N = len(trace.data)\n",
    "    yf = fft(trace.data)\n",
    "    xf = fftfreq(N, 1 / sampling_rate)\n",
    "\n",
    "    # Find the peak frequency\n",
    "    peak_freq = np.abs(xf[np.argmax(np.abs(yf))])\n",
    "\n",
    "    # Assume a wave speed (for example, P-wave speed in crust)\n",
    "    wave_speed = 6000  # in meters per second\n",
    "\n",
    "    # Calculate wavelength\n",
    "    wavelength = wave_speed / peak_freq\n",
    "\n",
    "    #print(f\"Peak Frequency: {peak_freq:.2f} Hz\")\n",
    "    #print(f\"Wavelength: {wavelength:.2f} meters\")\n",
    "\n",
    "    # Calculate RMS\n",
    "    rms_amplitude = np.sqrt(np.mean(trace.data**2))\n",
    "\n",
    "    # Estimate energy (empirical relationship)\n",
    "    energy_estimate = rms_amplitude**2  # Simplified relationship\n",
    "\n",
    "    #print(f\"RMS Amplitude: {rms_amplitude}\")\n",
    "    #print(f\"Estimated Energy (relative): {energy_estimate}\")\n",
    "    \n",
    "    details[\"RMS\"] = rms_amplitude\n",
    "    details[\"Energy\"] = energy_estimate\n",
    "    details[\"peak_freq\"] = peak_freq\n",
    "    details[\"wavelength\"] = wavelength\n",
    "    \n",
    "    time_ref = 3 #secs\n",
    "    tau_c_rep = []  # to store tau_c values\n",
    "\n",
    "    for i in range(len(combined_peaks)):\n",
    "        # Define the time window based on combined_peaks\n",
    "        t_window_start = int(combined_peaks[i])  # start at the filtered peak\n",
    "        t_window_end = min(len(raw_data_sliced), int(combined_peaks[i] + time_ref * sampling_rate))  # 3-10 seconds duration\n",
    "        \n",
    "        # Create time vector for this window (assuming sampling_rate is in Hz)\n",
    "        t_window = np.linspace(t_window_start / sampling_rate, t_window_end / sampling_rate, t_window_end - t_window_start)\n",
    "\n",
    "        # Extract the velocity data for the current time window\n",
    "        if t_window_end <= len(raw_data_sliced):  # Ensure window doesn't go out of bounds\n",
    "            v_t_window = raw_data_sliced[t_window_start:t_window_end]\n",
    "        else:\n",
    "            v_t_window = None  # Avoid undefined velocity window\n",
    "            logging.error(f\"Indices for v_t_window {t_window_start}, {t_window_end} are out of bounds.\")\n",
    "        if v_t_window is not None and len(v_t_window) > 0:    \n",
    "            # Calculate r using the extracted velocity data\n",
    "            numerator = simpson(v_t_window**2, x=t_window)  # Integral of velocity squared\n",
    "            denominator = simpson(v_t_window, x=t_window)   # Integral of velocity\n",
    "\n",
    "            # Calculate r (should take the absolute value of denominator to avoid negative values)\n",
    "            r = numerator / abs(denominator)\n",
    "\n",
    "            # Calculate tau_c (characteristic time constant)\n",
    "            tau_c = 2 * np.pi / np.sqrt(r)\n",
    "            tau_c_rep.append(tau_c)\n",
    "\n",
    "            # Output the results for r and tau_c\n",
    "            #print(f\"r {i+1}: {r}\")\n",
    "            #print(f\"tau_c {i+1}: {tau_c} seconds\")\n",
    "                \n",
    "            # First Integration: Velocity → Displacement using Simpson's Rule\n",
    "            # Use cumulative sum to integrate over time, giving a time-series of displacement.\n",
    "            displacement_t = np.cumsum(v_t_window) * (t_window[1] - t_window[0])\n",
    "\n",
    "            # Second Integration: Displacement → Moment History using Simpson's Rule\n",
    "            # Integrate the displacement time-series to get the moment history\n",
    "            moment_history_t_simpson = simpson(displacement_t, x=t_window)\n",
    "\n",
    "            # Output the results for moment history\n",
    "            #print(f\"Moment History (using Simpson's Rule): {moment_history_t_simpson}\")\n",
    "                \n",
    "            details[i+1][\"r\"] = r\n",
    "            details[i+1][\"moment_history\"] = moment_history_t_simpson\n",
    "            details[i+1][\"tau_c\"]=tau_c\n",
    "            \n",
    "            # Calculate maximum displacement amplitude\n",
    "            max_velocity_amplitude = np.max(np.abs(v_t_window))  # Max velocity amplitude\n",
    "            displacement_amplitude = max_velocity_amplitude / (2 * np.pi * dominant_frequency)  # Displacement amplitude\n",
    "            details[i + 1][\"Vel Amp (m/s)\"] = max_velocity_amplitude\n",
    "            details[i + 1][\"Disp Amp (m)\"] = displacement_amplitude\n",
    "        \n",
    "        else:\n",
    "            details[i + 1] = {\n",
    "                \"r\": None,\n",
    "                \"moment_history\": None,\n",
    "                \"tau_c\": None,\n",
    "                \"Vel Amp (m/s)\": None,\n",
    "                \"Disp Amp (m)\": None\n",
    "            }\n",
    "            # Optionally log or print a message if the velocity window is invalid\n",
    "            logging.warning(f\"Invalid velocity window for peak {i + 1}.\")\n",
    "            \n",
    "            \n",
    "            # Assuming you already have velocity data and time vector\n",
    "            # Example for displacement calculation (from velocity) using cumulative sum\n",
    "        displacement_t = np.cumsum(v_t_window) * (t_window[1] - t_window[0])  # Displacement after integrating velocity\n",
    "\n",
    "            # Find the first peak in the displacement signal after P-wave arrival\n",
    "            # Let's assume combined_peaks[i] is the P-wave arrival time index\n",
    "            # We look for peaks in the displacement after P-wave arrival (i.e., after combined_peaks[i])\n",
    "        peak_start = int(combined_peaks[i] + 1)  # Skip the initial P-wave\n",
    "        displacement_peaks, _ = find_peaks(displacement_t)\n",
    "                # Check if any peaks were found in displacement_t\n",
    "        if len(displacement_peaks) > 0:\n",
    "                # First peak in the displacement_t slice (at index 0 in the normalized slice)\n",
    "                first_peak_index = displacement_peaks[0]\n",
    "                first_peak_displacement = displacement_t[first_peak_index]\n",
    "                    \n",
    "                    # Now, let's calculate the scalar moment M0 using the first peak's displacement:\n",
    "                mu = 30e9  # Rigidity (in Pa, for most tectonic plates)\n",
    "                A = 10**7    # Fault area (example, in m^2, assuming 1 km²)\n",
    "                    \n",
    "                    # Calculate the scalar moment M0\n",
    "                M0 = mu * A * first_peak_displacement  # Scalar moment in Newton-meters (Nm)\n",
    "                    #print(f\"Scalar Moment (M0): {M0} Nm\")\n",
    "                details[\"peak_disp\"] = first_peak_displacement\n",
    "                details[\"M0\"] = M0\n",
    "                   \n",
    "                return details \n",
    "        else:\n",
    "                logging.info(\"No peak found\")\n",
    "                return details\n",
    "                \n",
    "def warn_check(st, time_window):\n",
    "    logging.info(f\"Starting warn_check with {len(st)} traces\")\n",
    "    ## last 4mins (240s) -> data_buffer\n",
    "    # Merge all traces in the data_buffer into a single Stream object\n",
    "                   \n",
    "    st2 = st.copy()\n",
    "                    \n",
    "    event_stat = {}\n",
    "                    \n",
    "    # Process components\n",
    "    st_z = st.select(component=\"Z\")\n",
    "    st_n = st.select(component=\"N\")\n",
    "    st_e = st.select(component=\"E\")\n",
    "\n",
    "    logging.info(f\"st_z: {st_z}, st_n: {st_n}, st_e: {st_e}\")\n",
    "\n",
    "    st2_z = st2.select(component=\"Z\")\n",
    "    st2_n = st2.select(component=\"N\")\n",
    "    st2_e = st2.select(component=\"E\")\n",
    "    \n",
    "    logging.info(f\"st2_z: {st2_z}, st2_n: {st2_n}, st2_e: {st2_e}\")\n",
    "    \n",
    "    filtered_z, time_vector_z = filtering_data(st2_z[0])\n",
    "    filtered_n, time_vector_n = filtering_data(st2_n[0])\n",
    "    filtered_e, time_vector_e = filtering_data(st2_e[0])\n",
    "    logging.info(f\"Filtered_z: {filtered_z}\")\n",
    "    # Process Z component\n",
    "    if len(st_z) > 0:\n",
    "        dom_freq_z, amp_z = fourier_transform(filtered_z, st_z[0])\n",
    "        event_stat[\"Domin_freq_z\"] = dom_freq_z\n",
    "        logging.info(f\"dom_freq_z: {dom_freq_z}\")\n",
    "    else:\n",
    "        print(\"No Z component traces available.\")\n",
    "\n",
    "    # Process N component\n",
    "    if len(st_n) > 0:\n",
    "        dom_freq_n, amp_n = fourier_transform(filtered_n, st_n[0])\n",
    "        event_stat[\"Domin_freq_n\"] = dom_freq_n\n",
    "    else:\n",
    "        print(\"No N component traces available.\")\n",
    "\n",
    "    # Process E component\n",
    "    if len(st_e) > 0:\n",
    "        dom_freq_e, amp_e = fourier_transform(filtered_e, st_e[0])\n",
    "        event_stat[\"Domin_freq_e\"] = dom_freq_e\n",
    "    else:\n",
    "        print(\"No E component traces available.\")\n",
    "                        \n",
    "    details = main_processing(filtered_z, time_window, dom_freq_z)\n",
    "    # Check if details is None, and handle accordingly\n",
    "    if details is None:\n",
    "        logging.error(\"No details returned from main_processing.\")\n",
    "        return\n",
    "    \n",
    "    for key, dicts in details.items():\n",
    "        if key == 1:\n",
    "            for stat, values in dicts.items():\n",
    "                if stat == \"Vel Amp (m/s)\":\n",
    "                    event_stat[\"P-Vel amp (m/s)\"] = values\n",
    "                elif stat == \"Disp Amp (m)\":\n",
    "                    event_stat[\"P-Disp amp (m)\"] = values\n",
    "                elif stat == \"peak2peak\":\n",
    "                    event_stat[\"P-peak2peak\"] = values\n",
    "                elif stat == \"r\":\n",
    "                    event_stat[\"P-r\"] = values\n",
    "                elif stat == \"moment_history\":\n",
    "                    event_stat[\"P-moment_history\"] = values\n",
    "                elif stat == \"tau_c\":\n",
    "                    event_stat[\"P-tau_c\"] = values    \n",
    "                else:\n",
    "                    print(f\"{key} error\")\n",
    "        elif key == 2:\n",
    "            for stat, values in dicts.items():\n",
    "                if stat == \"Vel Amp (m/s)\":\n",
    "                    event_stat[\"S-Vel amp (m/s)\"] = values\n",
    "                elif stat == \"Disp Amp (m)\":\n",
    "                    event_stat[\"S-Disp amp (m)\"] = values\n",
    "                elif stat == \"peak2peak\":\n",
    "                    event_stat[\"S-peak2peak\"] = values\n",
    "                elif stat == \"r\":\n",
    "                    event_stat[\"S-r\"] = values\n",
    "                elif stat == \"moment_history\":\n",
    "                    event_stat[\"S-moment_history\"] = values\n",
    "                elif stat == \"tau_c\":\n",
    "                    event_stat[\"S-tau_c\"] = values    \n",
    "                else:\n",
    "                    print(f\"{key} error\")\n",
    "        elif key == \"RMS\":\n",
    "            event_stat[\"RMS\"] = dicts    \n",
    "        elif key == \"Energy\":\n",
    "            event_stat[\"Energy\"] = dicts \n",
    "        elif key == \"peak_freq\":\n",
    "            event_stat[\"peak_freq\"] = dicts \n",
    "        elif key == \"wavelength\":\n",
    "            event_stat[\"wavelength\"] = dicts \n",
    "        elif key == \"peak_disp\":\n",
    "            event_stat[\"peak_disp\"] = dicts \n",
    "        elif key == \"M0\":\n",
    "            event_stat[\"M0\"] = dicts \n",
    "        else:\n",
    "            event_stat[\"other infos\"] = dicts\n",
    "                    \n",
    "        event_data = pd.DataFrame()\n",
    "        event_data = pd.concat([event_data, pd.Series(event_stat, name=key).to_frame().T], ignore_index=True, axis=0)\n",
    "        event_data.reset_index(drop=True, inplace=True)\n",
    "        event_data[\"Domin_freq_n\"] = abs(event_data[\"Domin_freq_n\"])\n",
    "        event_data[\"Domin_freq_e\"] = abs(event_data[\"Domin_freq_e\"])\n",
    "        event_data[\"Domin_freq_z\"] = abs(event_data[\"Domin_freq_z\"])\n",
    "        event_data[\"P-Vel amp (m/s)\"] = abs(event_data[\"P-Vel amp (m/s)\"])\n",
    "        if \"S-Vel amp (m/s)\" in event_data.columns:\n",
    "            event_data[\"S-Vel amp (m/s)\"] = abs(event_data[\"S-Vel amp (m/s)\"])\n",
    "        else:\n",
    "            logging.info(\"Column 'S-Vel amp (m/s)' not found!\")\n",
    "        event_data[\"P-Disp amp (m)\"] = abs(event_data[\"P-Disp amp (m)\"])\n",
    "        if \"S-Disp amp (m)\" in event_data.columns:\n",
    "            event_data[\"S-Disp amp (m)\"] = abs(event_data[\"S-Disp amp (m)\"])\n",
    "        else:\n",
    "            logging.info(\"Column 'S-Disp amp (m)' not found!\")            \n",
    "            \n",
    "        required_cols = ['Domin_freq_z','P-peak2peak', 'P-Disp amp (m)', 'P-r', 'P-moment_history', 'P-tau_c','RMS','Energy']\n",
    "        #error avoidance\n",
    "        missing_columns = [col for col in required_cols if col not in event_data.columns]\n",
    "        for col in missing_columns:\n",
    "            event_data[col] = np.nan\n",
    "            \n",
    "        features = event_data [['Domin_freq_z','P-peak2peak', 'P-Disp amp (m)', 'P-r', 'P-moment_history', 'P-tau_c','RMS','Energy']]\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        # normalize\n",
    "        normalizer = MinMaxScaler()\n",
    "        normalizer.fit(features)\n",
    "        features_norm = normalizer.transform(features)\n",
    "        features_norm = pd.DataFrame(features_norm, columns=features.columns)\n",
    "                    \n",
    "        return features_norm\n",
    "\n",
    "                        \n",
    "\n",
    "def process_trace(queue):\n",
    "    time_window = 300  # Time window in seconds (4 minutes)\n",
    "    data_buffer = deque()  # Create a deque to hold traces (ordered by time)\n",
    "    current_time = 0  # Track the time of the last trace (in seconds)\n",
    "    \n",
    "    try:\n",
    "        while True:\n",
    "            # Check if there is a new trace from the pipe\n",
    "            \n",
    "            if not queue.empty():  # Check if there's data available in the pipe\n",
    "                logging.info(\"P3- Processing initiated...\")\n",
    "                trace_record = queue.get()  # Receive the trace from P3\n",
    "                \n",
    "                logging.info(f\"P3 - Received new trace: {trace_record}, initiating processing...\")                              \n",
    "                             \n",
    "                st = trace_record.copy()\n",
    "                \n",
    "                new_trace_starttime = st[0].stats.starttime        \n",
    "                 # Add the new trace to the data buffer with the associated timestamp\n",
    "                data_buffer.append(st)\n",
    "                \n",
    "                while len(data_buffer) > 1:\n",
    "                    # Pop the oldest trace from the buffer (out of the time window)\n",
    "                    oldest_trace_starttime = data_buffer[0][0].stats.starttime\n",
    "                    # Check if starttime is a UTCDateTime or a Unix timestamp (float)\n",
    "                    if isinstance(new_trace_starttime, float):\n",
    "                        new_trace_starttime = time.gmtime(new_trace_starttime)\n",
    "                    if isinstance(oldest_trace_starttime, float):\n",
    "                        oldest_trace_starttime = time.gmtime(oldest_trace_starttime)\n",
    "                    # If they are `UTCDateTime` objects, subtract and check the difference\n",
    "                    if isinstance(new_trace_starttime, float) or isinstance(oldest_trace_starttime, float):\n",
    "                        # If float (Unix timestamp), calculate the difference in seconds directly\n",
    "                        time_difference = new_trace_starttime - oldest_trace_starttime\n",
    "                    else:\n",
    "                        # Subtract `UTCDateTime` objects and get the difference in seconds\n",
    "                        time_difference = new_trace_starttime - oldest_trace_starttime\n",
    "                    \n",
    "                    # Compare time difference with time window (in seconds)\n",
    "                    if time_difference > time_window:\n",
    "                        # Remove the oldest trace from the buffer if it exceeds the time window\n",
    "                        data_buffer.popleft()\n",
    "                    else:\n",
    "                        break  # Stop if the time window is valid\n",
    "                                 \n",
    "                if len(data_buffer) >= 3:\n",
    "                    #logging.info(f\"Data_buffer contains {len(data_buffer)} traces: {data_buffer}\")\n",
    "                    st_merged = Stream()\n",
    "                    for trace in data_buffer:\n",
    "                        st_merged += trace  # Merge each trace into the Stream\n",
    "                    st_merged.sort()\n",
    "                    st_merged.merge()\n",
    "                    logging.info(f\"st_merged: {st_merged}\")\n",
    "                    # Ensure that the time difference between the first and last trace is >= time_window\n",
    "                    oldest_trace_starttime = data_buffer[0][0].stats.starttime\n",
    "                    latest_trace_starttime = st_merged[-1].stats.endtime\n",
    "                    time_diff = (latest_trace_starttime - oldest_trace_starttime)\n",
    "\n",
    "                    if time_diff >= time_window:\n",
    "                        logging.info(f\"Time window condition met: {time_diff} seconds\")\n",
    "                        logging.info(f\"Processing {len(data_buffer)} traces within the last {time_window} seconds...\")\n",
    "\n",
    "                        # Call the warn_check function to process the traces\n",
    "                        features_norm = warn_check(st_merged, time_window)\n",
    "                        \n",
    "                        if features_norm is None:\n",
    "                            logging.error(\"warn_check returned None. Skipping processing for this batch of traces.\")\n",
    "                            continue  # Skip this batch and continue with the next set of traces\n",
    "\n",
    "                        # Load the model from the .pkl file\n",
    "                        model = joblib.load('models/model_FINAL.pkl')\n",
    "                        best_model = model.best_estimator_\n",
    "\n",
    "                        # Perform predictions\n",
    "                        for index, event in features_norm.iterrows():\n",
    "                            y_prob = best_model.predict_proba([event])[:, 1]  # Probabilities for the positive class\n",
    "                            custom_threshold = 0.01\n",
    "                            y_pred_custom_threshold = (y_prob >= custom_threshold).astype(int)\n",
    "                            if y_pred_custom_threshold == 1:\n",
    "                                logging.error(\">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> ALERT! <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\")\n",
    "                            else:\n",
    "                                logging.error(\"xxxxxxxxxxxxxxxxx  Nothing to report! xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \")\n",
    "                    \n",
    "                    else:\n",
    "                        logging.info(f\"Waiting for enough time window. Time difference: {time_difference} seconds\")\n",
    "\n",
    "                else:\n",
    "                    logging.info(f\"Waiting for 3 traces. Current trace count: {len(data_buffer)}\")\n",
    "\n",
    "            else:\n",
    "                time.sleep(1)  # Avoid busy waiting \n",
    "                \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in process_trace: {e}\", exc_info=True)  # Log with traceback\n",
    "\n",
    "    logging.info(\"Finished processing all traces.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
